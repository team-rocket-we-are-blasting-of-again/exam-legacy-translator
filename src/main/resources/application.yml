spring:
  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}

com:
  teamrocket:
    kafka:
      kafka-connect-host: ${KAFKA_CONNECT_HOST:localhost}
      kafka-connect-port: ${KAFKA_CONNECT_PORT:8083}
      connectors:
        # Source legacy restaurants
        jdbc-source-legacy-restaurants:
          connector.class: io.confluent.connect.jdbc.JdbcSourceConnector
          connection.url: ${LEGACY_DB_URL:jdbc:postgresql://postgres:5432/food_delivery}
          connection.user: ${LEGACY_DB_USERNAME:postgres}
          connection.password: ${LEGACY_DB_PASSWORD:Qwerty!234}
          dialect.name: PostgreSqlDatabaseDialect
          topic.prefix: legacy-restaurants
          transforms: createKey,setSchema
          transforms.createKey.type: org.apache.kafka.connect.transforms.ValueToKey
          transforms.createKey.fields: id
          transforms.setSchema.type: org.apache.kafka.connect.transforms.SetSchemaMetadata$Value
          transforms.setSchema.schema.name: OrderRecord
          poll.interval.ms: 5000
          mode: incrementing
          incrementing.column.name: id
          numeric.precision.mapping: true
          numeric.mapping: best_fit
          query: SELECT * FROM (SELECT id, restaurant_name, user_id FROM restaurants) AS RESTAURANT_DATA
          table.type: TABLE
          batch.max.rows: 500
        # Sink the legacy restaurants into new restaurants
        jdbc-sink-order-restaurants-restaurant-db:
          connector.class: io.confluent.connect.jdbc.JdbcSinkConnector
          topics: legacy-restaurants
          connection.url: ${RESTAURANT_DB_URL:jdbc:postgresql://restaurant-postgres:5432/restaurants}
          connection.user: ${RESTAURANT_DB_USERNAME:postgres}
          connection.password: ${RESTAURANT_DB_PASSWORD:Qwerty!234}
          table.name.format: restaurants
          transforms: Flatten, InsertArchived, InsertArea, Cast, RenameFields
          transforms.Flatten.type: org.apache.kafka.connect.transforms.Flatten$Value
          transforms.Flatten.delimiter: _
          transforms.InsertArchived.type: org.apache.kafka.connect.transforms.InsertField$Value
          transforms.InsertArchived.static.field: archived
          transforms.InsertArchived.static.value: false
          transforms.InsertArea.type: org.apache.kafka.connect.transforms.InsertField$Value
          transforms.InsertArea.static.field: area_id
          transforms.InsertArea.static.value: CPH
          transforms.Cast.type: org.apache.kafka.connect.transforms.Cast$Value
          transforms.Cast.spec: archived:boolean
          transforms.RenameFields.type: org.apache.kafka.connect.transforms.ReplaceField$Value
          transforms.RenameFields.renames: id:legacy_id,restaurant_name:restaurant_name,user_id:legacy_user_id,archived:archived,area_id:area_id
          insert.mode: upsert
          batch.size: 500
          pk.mode: record_value
          pk.fields: legacy_id
        # Source the legacy items
        jdbc-source-legacy-items:
          connector.class: io.confluent.connect.jdbc.JdbcSourceConnector
          connection.url: ${LEGACY_DB_URL:jdbc:postgresql://postgres:5432/food_delivery}
          connection.user: ${LEGACY_DB_USERNAME:postgres}
          connection.password: ${LEGACY_DB_PASSWORD:Qwerty!234}
          dialect.name: PostgreSqlDatabaseDialect
          topic.prefix: legacy-items
          transforms: createKey,setSchema
          transforms.createKey.type: org.apache.kafka.connect.transforms.ValueToKey
          transforms.createKey.fields: id
          transforms.setSchema.type: org.apache.kafka.connect.transforms.SetSchemaMetadata$Value
          transforms.setSchema.schema.name: LegacyItemRecord
          poll.interval.ms: 5000
          mode: incrementing
          incrementing.column.name: id
          numeric.precision.mapping: true
          numeric.mapping: best_fit
          query: SELECT * FROM (SELECT id, category, description, name, price, restaurant_id FROM items) AS ITEM_DATA
          table.type: TABLE
          batch.max.rows: 500
        # Sink the legacy items into new restaurant items
        jdbc-sink-items-restaurant-db:
          connector.class: io.confluent.connect.jdbc.JdbcSinkConnector
          topics: legacy-items
          connection.url: ${RESTAURANT_DB_URL:jdbc:postgresql://restaurant-postgres:5432/restaurants}
          connection.user: ${RESTAURANT_DB_USERNAME:postgres}
          connection.password: ${RESTAURANT_DB_PASSWORD:Qwerty!234}
          table.name.format: items
          transforms: Flatten, RenameFields
          transforms.Flatten.type: org.apache.kafka.connect.transforms.Flatten$Value
          transforms.Flatten.delimiter: _
          transforms.RenameFields.type: org.apache.kafka.connect.transforms.ReplaceField$Value
          transforms.RenameFields.renames: id:legacy_id,category:category,description:description,name:name,price:price,restaurant_id:legacy_restaurant_id
          insert.mode: upsert
          batch.size: 500
          pk.mode: record_value
          pk.fields: legacy_id
        # Source legacy orders
        jdbc-source-legacy-orders:
          connector.class: io.confluent.connect.jdbc.JdbcSourceConnector
          connection.url: ${LEGACY_DB_URL:jdbc:postgresql://postgres:5432/food_delivery}
          connection.user: ${LEGACY_DB_USERNAME:postgres}
          connection.password: ${LEGACY_DB_PASSWORD:Qwerty!234}
          dialect.name: PostgreSqlDatabaseDialect
          topic.prefix: legacy-orders
          transforms: createKey,setSchema
          transforms.createKey.type: org.apache.kafka.connect.transforms.ValueToKey
          transforms.createKey.fields: id
          transforms.setSchema.type: org.apache.kafka.connect.transforms.SetSchemaMetadata$Value
          transforms.setSchema.schema.name: OrderRecord
          poll.interval.ms: 5000
          mode: incrementing
          incrementing.column.name: id
          numeric.precision.mapping: true
          numeric.mapping: best_fit
          query: SELECT * FROM (SELECT id, order_date, total_price, delivered_by, customer_id, restaurant_id FROM orders) AS ORDER_DATA
          table.type: TABLE
          batch.max.rows: 500
        # Sink legacy orders to restaurant orders
        jdbc-sink-orders-restaurant-db:
          connector.class: io.confluent.connect.jdbc.JdbcSinkConnector
          topics: legacy-orders
          connection.url: ${RESTAURANT_DB_URL:jdbc:postgresql://restaurant-postgres:5432/restaurants}
          connection.user: ${RESTAURANT_DB_USERNAME:postgres}
          connection.password: ${RESTAURANT_DB_PASSWORD:Qwerty!234}
          table.name.format: restaurant_orders
          transforms: Flatten, InsertStatus, InsertDelivery, Cast, RenameFields
          transforms.Flatten.type: org.apache.kafka.connect.transforms.Flatten$Value
          transforms.Flatten.delimiter: _
          transforms.InsertStatus.type: org.apache.kafka.connect.transforms.InsertField$Value
          transforms.InsertStatus.static.field: status
          transforms.InsertStatus.static.value: COMPLETED
          transforms.InsertDelivery.type: org.apache.kafka.connect.transforms.InsertField$Value
          transforms.InsertDelivery.static.field: with_delivery
          transforms.InsertDelivery.static.value: true
          transforms.Cast.type: org.apache.kafka.connect.transforms.Cast$Value
          transforms.Cast.spec: with_delivery:boolean
          transforms.RenameFields.type: org.apache.kafka.connect.transforms.ReplaceField$Value
          transforms.RenameFields.renames: id:legacy_id,order_date:created_at,total_price:total_price,restaurant_id:legacy_restaurant_id,status:status,with_delivery:with_delivery
          transforms.RenameFields.exclude: customer_id,delivered_by
          insert.mode: upsert
          batch.size: 5
          pk.mode: record_value
          pk.fields: legacy_id
        # Source the restaurant id and matching item id from the newly inserted data
        jdbc-source-restaurants-id-mapping:
          connector.class: io.confluent.connect.jdbc.JdbcSourceConnector
          connection.url: ${RESTAURANT_DB_URL:jdbc:postgresql://restaurant-postgres:5432/restaurants}
          connection.user: ${RESTAURANT_DB_USERNAME:postgres}
          connection.password: ${RESTAURANT_DB_PASSWORD:Qwerty!234}
          dialect.name: PostgreSqlDatabaseDialect
          topic.prefix: restaurants-id-mapping
          transforms: createKey,setSchema
          transforms.createKey.type: org.apache.kafka.connect.transforms.ValueToKey
          transforms.createKey.fields: id
          transforms.setSchema.type: org.apache.kafka.connect.transforms.SetSchemaMetadata$Value
          transforms.setSchema.schema.name: RestaurantIdMapping
          poll.interval.ms: 5000
          mode: incrementing
          incrementing.column.name: id
          numeric.precision.mapping: true
          numeric.mapping: best_fit
          query: SELECT * FROM (SELECT r.id, i.id AS item_id, o.id as order_id FROM restaurants r INNER JOIN items i ON r.legacy_id = i.legacy_restaurant_id INNER JOIN restaurant_orders o ON r.legacy_id = o.legacy_restaurant_id WHERE r.legacy_id IS NOT NULL AND o.legacy_restaurant_id IS NOT NULL) AS RESTAURANT_ID_MAPPING
          table.type: TABLE
          batch.max.rows: 500
        # Update the restaurant items, to correctly set the referenced restaurant_id column
        jdbc-sink-ids-from-restaurant-mapping-to-items:
          connector.class: io.confluent.connect.jdbc.JdbcSinkConnector
          topics: restaurants-id-mapping
          connection.url: ${RESTAURANT_DB_URL:jdbc:postgresql://restaurant-postgres:5432/restaurants}
          connection.user: ${RESTAURANT_DB_USERNAME:postgres}
          connection.password: ${RESTAURANT_DB_PASSWORD:Qwerty!234}
          table.name.format: items
          transforms: Flatten, RenameFields
          transforms.Flatten.type: org.apache.kafka.connect.transforms.Flatten$Value
          transforms.Flatten.delimiter: _
          transforms.RenameFields.type: org.apache.kafka.connect.transforms.ReplaceField$Value
          transforms.RenameFields.renames: id:restaurant_id,item_id:id
          fields.whitelist: restaurant_id,id
          insert.mode: update
          batch.size: 500
          pk.mode: record_value
          pk.fields: id
        # Update the orders restaurant id to be correct
        jdbc-sink-ids-from-restaurant-mapping-to-orders:
          connector.class: io.confluent.connect.jdbc.JdbcSinkConnector
          topics: restaurants-id-mapping
          connection.url: ${RESTAURANT_DB_URL:jdbc:postgresql://restaurant-postgres:5432/restaurants}
          connection.user: ${RESTAURANT_DB_USERNAME:postgres}
          connection.password: ${RESTAURANT_DB_PASSWORD:Qwerty!234}
          table.name.format: restaurant_orders
          transforms: Flatten, RenameFields
          transforms.Flatten.type: org.apache.kafka.connect.transforms.Flatten$Value
          transforms.Flatten.delimiter: _
          transforms.RenameFields.type: org.apache.kafka.connect.transforms.ReplaceField$Value
          transforms.RenameFields.renames: id:restaurant_id,order_id:id
          fields.whitelist: restaurant_id,id
          insert.mode: update
          batch.size: 500
          pk.mode: record_value
          pk.fields: id
        # Source legacy order_items
        jdbc-source-legacy-order-items:
          connector.class: io.confluent.connect.jdbc.JdbcSourceConnector
          connection.url: ${LEGACY_DB_URL:jdbc:postgresql://postgres:5432/food_delivery}
          connection.user: ${LEGACY_DB_USERNAME:postgres}
          connection.password: ${LEGACY_DB_PASSWORD:Qwerty!234}
          dialect.name: PostgreSqlDatabaseDialect
          topic.prefix: legacy-order-items
          transforms: setSchema
          transforms.setSchema.type: org.apache.kafka.connect.transforms.SetSchemaMetadata$Value
          transforms.setSchema.schema.name: OrderItemsRecord
          poll.interval.ms: 5000
          mode: bulk
          numeric.mapping: best_fit
          query: SELECT * FROM (SELECT order_id, quantity, item_id FROM order_items) AS ORDER_ITEMS_DATA
          table.type: TABLE
          batch.max.rows: 500
        # Sink legacy order_items into legacy_order_items table only used for mapping.
        jdbc-sink-order-items-restaurant-db:
          connector.class: io.confluent.connect.jdbc.JdbcSinkConnector
          topics: legacy-order-items
          connection.url: ${RESTAURANT_DB_URL:jdbc:postgresql://restaurant-postgres:5432/restaurants}
          connection.user: ${RESTAURANT_DB_USERNAME:postgres}
          connection.password: ${RESTAURANT_DB_PASSWORD:Qwerty!234}
          table.name.format: legacy_order_items
          transforms: Flatten, RenameFields
          transforms.Flatten.type: org.apache.kafka.connect.transforms.Flatten$Value
          transforms.Flatten.delimiter: _
          transforms.RenameFields.type: org.apache.kafka.connect.transforms.ReplaceField$Value
          transforms.RenameFields.renames: order_id:legacy_order_id,quantity:quantity,item_id:legacy_item_id
          insert.mode: upsert
          batch.size: 500
          pk.mode: record_value
          pk.fields: legacy_order_id,legacy_item_id
          auto.create: true
          auto.evolve: true
        # Source that gives us order_id, item_id and quantity from the legacy_order_id and legacy_item_id
        jdbc-source-order-items-id-mapping:
          connector.class: io.confluent.connect.jdbc.JdbcSourceConnector
          connection.url: ${RESTAURANT_DB_URL:jdbc:postgresql://restaurant-postgres:5432/restaurants}
          connection.user: ${RESTAURANT_DB_USERNAME:postgres}
          connection.password: ${RESTAURANT_DB_PASSWORD:Qwerty!234}
          dialect.name: PostgreSqlDatabaseDialect
          topic.prefix: order-items-id-mapping
          transforms: setSchema
          transforms.setSchema.type: org.apache.kafka.connect.transforms.SetSchemaMetadata$Value
          transforms.setSchema.schema.name: RestaurantIdMapping
          poll.interval.ms: 5000
          mode: bulk
          numeric.precision.mapping: true
          numeric.mapping: best_fit
          query: SELECT * FROM (SELECT o.id as order_id, i.id as item_id, oi.quantity FROM legacy_order_items oi INNER JOIN items i ON oi.legacy_item_id = i.legacy_id INNER JOIN restaurant_orders o ON oi.legacy_order_id = o.legacy_id WHERE i.legacy_id IS NOT NULL AND o.legacy_id IS NOT NULL) AS RESTAURANT_ID_MAPPING
          table.type: TABLE
          batch.max.rows: 500
        # Sink order_id, item_id and quantity into the restaurant order_items table
        jdbc-sink-correct-order-items-mapping:
          connector.class: io.confluent.connect.jdbc.JdbcSinkConnector
          topics: order-items-id-mapping
          connection.url: ${RESTAURANT_DB_URL:jdbc:postgresql://restaurant-postgres:5432/restaurants}
          connection.user: ${RESTAURANT_DB_USERNAME:postgres}
          connection.password: ${RESTAURANT_DB_PASSWORD:Qwerty!234}
          table.name.format: order_items
          transforms: Flatten, RenameFields
          transforms.Flatten.type: org.apache.kafka.connect.transforms.Flatten$Value
          transforms.Flatten.delimiter: _
          transforms.RenameFields.type: org.apache.kafka.connect.transforms.ReplaceField$Value
          transforms.RenameFields.renames: order_id:order_id,item_id:item_id,quantity:quantity
          insert.mode: upsert
          batch.size: 500
          pk.mode: record_value
          pk.fields: order_id,item_id
#        jdbc-source-orders-id-mapping:
#          connector.class: io.confluent.connect.jdbc.JdbcSourceConnector
#          connection.url: ${RESTAURANT_DB_URL:jdbc:postgresql://restaurant-postgres:5432/restaurants}
#          connection.user: ${RESTAURANT_DB_USERNAME:postgres}
#          connection.password: ${RESTAURANT_DB_PASSWORD:Qwerty!234}
#          dialect.name: PostgreSqlDatabaseDialect
#          topic.prefix: orders-id-mapping
#          transforms: createKey,setSchema
#          transforms.createKey.type: org.apache.kafka.connect.transforms.ValueToKey
#          transforms.createKey.fields: id
#          transforms.setSchema.type: org.apache.kafka.connect.transforms.SetSchemaMetadata$Value
#          transforms.setSchema.schema.name: OrdersIdMapping
#          poll.interval.ms: 5000
#          mode: incrementing
#          incrementing.column.name: id
#          numeric.precision.mapping: true
#          numeric.mapping: best_fit
#          query: SELECT * FROM (SELECT r.id, i.id AS item_id FROM restaurant_orders r INNER JOIN items i ON r.legacy_id = i.legacy_restaurant_id WHERE r.legacy_id IS NOT NULL) AS RESTAURANT_ID_MAPPING
#          table.type: TABLE
#          batch.max.rows: 500
#        jdbc-sink-orders-order-db:
#          connector.class: io.confluent.connect.jdbc.JdbcSinkConnector
#          topics: legacy-orders
#          connection.url: jdbc:postgresql://order-postgres:5432/orders
#          connection.user: postgres
#          connection.password: postgres
#          table.name.format: orders
#          transforms: Flatten, InsertStatus, InsertDelivery, Cast, RenameFields
#          transforms.Flatten.type: org.apache.kafka.connect.transforms.Flatten$Value
#          transforms.Flatten.delimiter: _
#          transforms.InsertStatus.type: org.apache.kafka.connect.transforms.InsertField$Value
#          transforms.InsertStatus.static.field: status
#          transforms.InsertStatus.static.value: COMPLETED
#          transforms.InsertDelivery.type: org.apache.kafka.connect.transforms.InsertField$Value
#          transforms.InsertDelivery.static.field: with_delivery
#          transforms.InsertDelivery.static.value: true
#          transforms.Cast.type: org.apache.kafka.connect.transforms.Cast$Value
#          transforms.Cast.spec: with_delivery:boolean
#          transforms.RenameFields.type: org.apache.kafka.connect.transforms.ReplaceField$Value
#          transforms.RenameFields.renames: id:legacy_id,order_date:created_at,total_price:delivery_price,delivered_by:legacy_courier_id,customer_id:legacy_customer_id,restaurant_id:legacy_restaurant_id,status:status,with_delivery:with_delivery
#          insert.mode: upsert
#          batch.size: 5
#          pk.mode: record_value
#          pk.fields: legacy_id
#        jdbc-sink-order-items-order-db:
#          connector.class: io.confluent.connect.jdbc.JdbcSinkConnector
#          topics: legacy-order_items
#          connection.url: jdbc:postgresql://order-postgres:5432/orders
#          connection.user: postgres
#          connection.password: postgres
#          table.name.format: order_item
#          transforms: Flatten, RenameFields
#          transforms.Flatten.type: org.apache.kafka.connect.transforms.Flatten$Value
#          transforms.Flatten.delimiter: _
#          transforms.RenameFields.type: org.apache.kafka.connect.transforms.ReplaceField$Value
#          transforms.RenameFields.renames: order_id:legacy_order_id,quantity:amount,item_id:legacy_menu_item_id
#          insert.mode: upsert
#          batch.size: 500
#          pk.mode: record_value
#          pk.fields: legacy_order_id,legacy_menu_item_id
        # Source legacy items
        # Sink to new items with legacy_id
        # Source from restaurants
        #
